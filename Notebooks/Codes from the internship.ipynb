{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Gradio for met data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import urllib.parse\n",
    "import gradio as gr\n",
    "from io import StringIO\n",
    "def get_elevation(lat, long):\n",
    "    \"\"\"\n",
    "      Fetch the elevation (in meters) for a given latitude and longitude using the Open-Meteo API.\n",
    "\n",
    "      Args:\n",
    "          lat (float): Latitude of the location for which elevation is required.\n",
    "          long (float): Longitude of the location for which elevation is required.\n",
    "\n",
    "      Returns:\n",
    "          float: Elevation in meters for the given latitude and longitude based on the Copernicus DEM 2021 release GLO-90 with 90 meters resolution.\n",
    "    \"\"\"\n",
    "    # Base URL for the Open-Meteo historical weather API\n",
    "    ELEVATION_API_BASE_URL = \"https://api.open-meteo.com/v1/elevation?\"\n",
    "\n",
    "    # Construct the URL parameters for the API request\n",
    "    ELEVATION_API_URL_PARAMS = {\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": long\n",
    "    }\n",
    "\n",
    "    # Create the full URL for the API request\n",
    "    url = ELEVATION_API_BASE_URL + urllib.parse.urlencode(ELEVATION_API_URL_PARAMS, doseq=True)\n",
    "\n",
    "    # Make the API request\n",
    "    response = requests.get(url, timeout=5)\n",
    "\n",
    "    # Get the real elevation\n",
    "    ELEVATION = response.json()\n",
    "\n",
    "    return ELEVATION['elevation']\n",
    "\n",
    "\n",
    "def get_met_data(lat_list, long_list, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch historical weather data from the Open-Meteo API for given locations and date ranges.\n",
    "\n",
    "    Parameters:\n",
    "    - lat_list (list of str): List of latitudes for desired locations.\n",
    "    - long_list (list of str): List of longitudes for desired locations.\n",
    "    - start_date (list of str): List of start dates in 'YYYY-MM-DD' format.\n",
    "    - end_date (list of str): List of end dates in 'YYYY-MM-DD' format.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Weather data for the given locations and date ranges.\n",
    "\n",
    "    For available variables, refer to:\n",
    "    https://open-meteo.com/en/docs/historical-weather-api#models=best_match\n",
    "    \"\"\"\n",
    "    # Base URL for the Open-Meteo historical weather API\n",
    "    WEATHER_API_BASE_URL = \"https://archive-api.open-meteo.com/v1/archive?\"\n",
    "\n",
    "    # Variables of interest for the daily data\n",
    "    VARIABLES = [\"precipitation_sum\", \"et0_fao_evapotranspiration\", \"weathercode\"]\n",
    "\n",
    "    # Initialize an empty DataFrame to store the results\n",
    "    met_data = pd.DataFrame()\n",
    "    lat_list = [lat_list]\n",
    "    long_list = [long_list ]\n",
    "    start_date = [start_date]\n",
    "    end_date = [end_date]\n",
    "    # Loop over each location and date range to fetch the data\n",
    "    for lat, long, start, end in zip(lat_list, long_list, start_date, end_date):\n",
    "        # Get elevation\n",
    "        ELEVATION = get_elevation(lat, long)\n",
    "\n",
    "        # Construct the URL parameters for the API request\n",
    "        WEATHER_API_URL_PARAMS = {\n",
    "            \"latitude\": lat,\n",
    "            \"longitude\": long,\n",
    "            \"start_date\": start,\n",
    "            \"end_date\": end,\n",
    "            \"daily\": VARIABLES,\n",
    "            #   \"hourly\": \"soil_temperature_0_to_7cm\",\n",
    "            \"elevation\": ELEVATION,\n",
    "            \"timezone\": \"GMT\"\n",
    "        }\n",
    "\n",
    "        # Create the full URL for the API request\n",
    "        url = WEATHER_API_BASE_URL + urllib.parse.urlencode(WEATHER_API_URL_PARAMS, doseq=True)\n",
    "        # Make the API request\n",
    "        response = requests.get(url, timeout=5)\n",
    "\n",
    "        # Parse the JSON response\n",
    "        data = response.json()\n",
    "\n",
    "        # Convert the daily data to a DataFrame and add the lat and long columns\n",
    "        precipitation_sum_3days = sum(\n",
    "            data[\"daily\"]['precipitation_sum'][-3:])  # Calculate the weather_code on the day before measurement\n",
    "        weather_code = data[\"daily\"]['weathercode'][-2]  # Calculate the weather_code on the day before measurement\n",
    "        current_data = pd.DataFrame.from_dict(data[\"daily\"])  # Create a df\n",
    "        current_data.drop(columns=[\"time\", \"weathercode\"], inplace=True)\n",
    "        current_data = current_data.sum(axis=0).to_frame().T  # Calculate the sum for two weeks\n",
    "\n",
    "        # Add values to DF\n",
    "        current_data[\"weathercode_day_before\"] = weather_code\n",
    "        current_data['precipitation_last_3days'] = precipitation_sum_3days\n",
    "        current_data[\"lat\"], current_data[\"long\"] = lat, long\n",
    "        current_data[\"elevation\"] = ELEVATION\n",
    "        current_data[\"date\"] = end\n",
    "        # sm = pd.DataFrame.from_dict(data[\"hourly\"])\n",
    "\n",
    "        # Append the current data to the main DataFrame\n",
    "        met_data = met_data._append(current_data).reset_index(drop=True)\n",
    "\n",
    "    # Convert the DF toa\n",
    "    # csv_buffer = StringIO()\n",
    "    # met_data.to_csv(csv_buffer, index=False)\n",
    "    # csv_str = csv_buffer.getvalue()\n",
    "    # Return the final DataFrame containing the weather data\n",
    "    return met_data,\n",
    "\n",
    "# Define the Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=get_met_data,\n",
    "    inputs=[\n",
    "        gr.Number(label='Latitude'),\n",
    "        gr.Number(label='Longitude'),\n",
    "        gr.Textbox(label='Start Date'),\n",
    "        gr.Textbox(label='End Date')\n",
    "    ],\n",
    "    outputs=gr.Dataframe(label=\"Generated CSV\"),\n",
    "    live=False  # Because we're generating a file, it's best to set live to False\n",
    ")\n",
    "iface.launch()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Elevation Data From Open-Meteo API"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_elevation(lat, long):\n",
    "    \"\"\"\n",
    "    Fetch the elevation (in meters) for a given latitude and longitude using the Open-Meteo API.\n",
    "\n",
    "    Args:\n",
    "        lat (float): Latitude of the location for which elevation is required.\n",
    "        long (float): Longitude of the location for which elevation is required.\n",
    "\n",
    "    Returns:\n",
    "        float: Elevation in meters for the given latitude and longitude.\n",
    "\n",
    "    Raises:\n",
    "        requests.RequestException: If there's an issue with the API request.\n",
    "        KeyError: If 'elevation' key is not present in the API response.\n",
    "\n",
    "    Example:\n",
    "        >>> get_elevation(34.0522, -118.2437)\n",
    "        71.0  # (This is a hypothetical result, actual output may vary.)\n",
    "\n",
    "    Note:\n",
    "        Ensure that the 'requests' and 'urllib.parse' modules are imported before using this function.\n",
    "    \"\"\"\n",
    "\n",
    "    # Base URL for the Open-Meteo historical weather API\n",
    "    ELEVATION_API_BASE_URL = \"https://api.open-meteo.com/v1/elevation?\"\n",
    "\n",
    "    # Construct the URL parameters for the API request\n",
    "    ELEVATION_API_URL_PARAMS = {\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": long\n",
    "    }\n",
    "\n",
    "    # Create the full URL for the API request\n",
    "    url = ELEVATION_API_BASE_URL + urllib.parse.urlencode(ELEVATION_API_URL_PARAMS, doseq=True)\n",
    "\n",
    "    # Make the API request\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Get the real elevation\n",
    "    ELEVATION = response.json()\n",
    "    return ELEVATION['elevation']\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Met Data From Open-Meteo API"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "def get_met_data(lat_list, long_list, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch historical weather data from the Open-Meteo API for given locations and date ranges.\n",
    "\n",
    "    Parameters:\n",
    "    - lat_list (list of str): List of latitudes for desired locations.\n",
    "    - long_list (list of str): List of longitudes for desired locations.\n",
    "    - start_date (list of str): List of start dates in 'YYYY-MM-DD' format.\n",
    "    - end_date (list of str): List of end dates in 'YYYY-MM-DD' format.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Weather data for the given locations and date ranges.\n",
    "\n",
    "    For available variables, refer to:\n",
    "    https://open-meteo.com/en/docs/historical-weather-api#models=best_match\n",
    "    \"\"\"\n",
    "\n",
    "    WEATHER_API_BASE_URL = \"https://archive-api.open-meteo.com/v1/archive?\"\n",
    "    VARIABLES = [\"precipitation_sum\", \"et0_fao_evapotranspiration\", \"weathercode\"]\n",
    "    met_data = pd.DataFrame()\n",
    "\n",
    "    for lat, long, start, end in zip(lat_list, long_list, start_date, end_date):\n",
    "        WEATHER_API_URL_PARAMS = {\n",
    "            \"latitude\": lat,\n",
    "            \"longitude\": long,\n",
    "            \"start_date\": start,\n",
    "            \"end_date\": end,\n",
    "            \"daily\": VARIABLES,\n",
    "            \"timezone\": \"GMT\"\n",
    "        }\n",
    "\n",
    "        url = WEATHER_API_BASE_URL + urllib.parse.urlencode(WEATHER_API_URL_PARAMS, doseq=True)\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        current_data = pd.DataFrame.from_dict(data[\"daily\"])\n",
    "        current_data[\"lat\"] = lat\n",
    "        current_data[\"long\"] = long\n",
    "        met_data = pd.concat([met_data, current_data], ignore_index=True)\n",
    "\n",
    "    return met_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    lat_list = [\"37.71008935\", \"52.1250655\"]\n",
    "    long_list = [\"-122.196217\", \"4.7563068\"]\n",
    "    start_date = [\"2023-04-01\", \"2023-04-01\"]\n",
    "    end_date = [\"2023-04-02\", \"2023-04-02\"]\n",
    "    data = get_met_data(lat_list, long_list, start_date, end_date)\n",
    "    print(data)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get soil Data from soilgrids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "import pandas as pd\n",
    "\n",
    "def soil_data_df(response, PARAMS):\n",
    "    \"\"\"\n",
    "    Converts API response into a DataFrame for the given soil data parameters.\n",
    "\n",
    "    Args:\n",
    "    - response (requests.Response): Response object from the ISRIC API.\n",
    "    - PARAMS (dict): Dictionary containing query parameters for the API call.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A single-row DataFrame containing the soil data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract JSON data from the API response\n",
    "    data = response.json()\n",
    "\n",
    "    # Determine number of layers and depths based on the parameters\n",
    "    LAYERS = len(PARAMS[\"property\"])\n",
    "    DEPTHS = len(PARAMS[\"depth\"])\n",
    "\n",
    "    row_data = {}  # Initialize an empty dictionary for this lat-long\n",
    "\n",
    "    # Extract the required soil data values based on the layers and depths\n",
    "    for layer in range(LAYERS):\n",
    "        for depth in range(DEPTHS):\n",
    "            current_depth = PARAMS[\"depth\"][depth]\n",
    "            current_property = PARAMS[\"property\"][layer]\n",
    "            value = data[\"properties\"]['layers'][layer]['depths'][depth]['values']['mean']\n",
    "            layer_name = str(current_property) + \" \" + str(current_depth)\n",
    "\n",
    "            row_data[layer_name] = value  # Assign the value to the dictionary\n",
    "\n",
    "    # Convert the nested dictionary to a DataFrame\n",
    "    current_lat_long = pd.DataFrame([row_data])\n",
    "\n",
    "    return current_lat_long\n",
    "\n",
    "def get_soil_data(lat_list, long_list):\n",
    "    \"\"\"\n",
    "    Fetches soil data from the ISRIC API for the given list of latitudes and longitudes.\n",
    "\n",
    "    Args:\n",
    "    - lat_list (list): List of latitudes.\n",
    "    - long_list (list): List of longitudes.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: DataFrame containing soil data for all the provided coordinates.\n",
    "    \"\"\"\n",
    "\n",
    "    BASE_URL = \"https://rest.isric.org/soilgrids/v2.0/properties/query?\"\n",
    "\n",
    "    # Define the soil properties, depths, and value type to query for\n",
    "    VARIABLES = [\"cfvo\", \"soc\"]\n",
    "    DEPTH = [\"0-5cm\", \"5-15cm\"]\n",
    "    VALUE = \"mean\"\n",
    "\n",
    "    # Initialize an empty DataFrame to store the results\n",
    "    soil_data = pd.DataFrame()\n",
    "\n",
    "    # Fetch soil data for each latitude-longitude pair\n",
    "    for lat, long in zip(lat_list, long_list):\n",
    "        PARAMS = {\n",
    "            \"lon\": long,\n",
    "            \"lat\": lat,\n",
    "            \"property\": VARIABLES,\n",
    "            \"depth\": DEPTH,\n",
    "            \"value\": VALUE\n",
    "        }\n",
    "        url = BASE_URL + urllib.parse.urlencode(PARAMS, doseq=True)\n",
    "\n",
    "        response = requests.get(url)\n",
    "\n",
    "        current_lat_long = soil_data_df(response, PARAMS)\n",
    "        soil_data = soil_data.append(current_lat_long, ignore_index=True)  # Corrected from '_append' to 'append'\n",
    "\n",
    "    return soil_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pd.set_option('display.max_columns', 7)\n",
    "    lat_list = [\"37.71008935\", \"52.1250655\"]\n",
    "    long_list = [\"-122.196217\", \"4.7563068\"]\n",
    "\n",
    "    soil_data = get_soil_data(lat_list, long_list)\n",
    "    print(soil_data)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Returns the date specified days prior to the provided date"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def starting_date(date, days=14):\n",
    "    \"\"\"\n",
    "    Returns the date of a specific number of days prior to the provided date.\n",
    "\n",
    "    This function takes a date in the format 'YYYY-MM-DD', calculates the date\n",
    "    based on the provided days (default is 14 days) earlier, and returns this earlier date in the same format.\n",
    "\n",
    "    Parameters:\n",
    "    - date (str): The input date string in the format 'YYYY-MM-DD'.\n",
    "    - days (int): Number of days to subtract from the provided date. Default is 14.\n",
    "\n",
    "    Returns:\n",
    "    - str: Date in the 'YYYY-MM-DD' format that represents a date 'days' prior\n",
    "           to the provided input date.\n",
    "\n",
    "    Examples:\n",
    "    >>> starting_date(\"2023-04-15\")\n",
    "    '2023-04-01'\n",
    "    \"\"\"\n",
    "    original_date = datetime.strptime(date, '%Y-%m-%d')\n",
    "    earlier_date = original_date - timedelta(days=days)\n",
    "    earlier_date_str = earlier_date.strftime('%Y-%m-%d')\n",
    "    return earlier_date_str\n",
    "\n",
    "# Sample data\n",
    "data = pd.DataFrame({\n",
    "    'date': ['2023-04-15', '2023-04-20', '2023-05-01']\n",
    "})\n",
    "\n",
    "# Get the 3 days before dates\n",
    "data[\"days_ago\"] = data[\"date\"].apply(starting_date, days=3)\n",
    "print(data)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "binning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the maximum value from the 'VWC' column\n",
    "max_value = met_soil_data['VWC'].max()\n",
    "if max_value < 70:\n",
    "    max_value = 70\n",
    "\n",
    "# Defining bin edges and labels\n",
    "bin_edges = [10, 20, 30, 40, 50, 60, 70, max_value]\n",
    "bin_labels = [\"10\", \"20\", \"30\", \"40\", \"50\", \"60\", \"70\", \">70\"]\n",
    "\n",
    "# Binning the data\n",
    "met_soil_data['VWC_category'] = pd.cut(met_soil_data['VWC'], bins=bin_edges, labels=bin_labels, include_lowest=True)\n",
    "\n",
    "# Display the original and binned columns\n",
    "met_soil_data[['VWC', 'VWC_category']].head()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get pixel values`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from osgeo.gdal import Open\n",
    "import time\n",
    "import rasterio\n",
    "\n",
    "def world_to_pixel_coordinates(geotransform, x, y):\n",
    "    \"\"\"\n",
    "    Convert world coordinates to pixel coordinates.\n",
    "\n",
    "    Args:\n",
    "        geotransform (tuple): Geotransformation parameters.\n",
    "        x (float): X-coordinate in world coordinates.\n",
    "        y (float): Y-coordinate in world coordinates.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Pixel coordinates (px, py).\n",
    "    \"\"\"\n",
    "    px = int((x - geotransform[0]) / geotransform[1])\n",
    "    py = int((y - geotransform[3]) / geotransform[5])\n",
    "    return px, py\n",
    "\n",
    "def get_pixel_value(dataset, px, py, band_number=1):\n",
    "    \"\"\"\n",
    "    Get pixel value from raster dataset at specified pixel coordinates.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): Raster dataset.\n",
    "        px (int): Pixel x-coordinate.\n",
    "        py (int): Pixel y-coordinate.\n",
    "        band_number (int, optional): Band number. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        int: Pixel value.\n",
    "    \"\"\"\n",
    "    band = dataset.GetRasterBand(band_number)  # Assuming you want the first band\n",
    "    return band.ReadAsArray(px, py, 1, 1)[0][0]\n",
    "\n",
    "# After converting to tiff with AGREF\n",
    "image_file_name = \"0000451468_001001_ALOS2411892820-220107/IMG-ALOS2411892820-220107-HBQL1_5RUD_OR_NN_dB_VH.tif\"\n",
    "dataset = Open(image_file_name)\n",
    "xsize = dataset.RasterXSize\n",
    "ysize = dataset.RasterYSize\n",
    "print(f\"Image size is {xsize} x {ysize}\")\n",
    "geotransform = dataset.GetGeoTransform()\n",
    "\n",
    "# Input world coordinates\n",
    "x, y = -122.196214, 37.71008935\n",
    "\n",
    "# Convert world coordinates to pixel coordinates using GDAL\n",
    "px, py = world_to_pixel_coordinates(geotransform, x, y)\n",
    "\n",
    "# Get pixel value using GDAL\n",
    "start_time = time.time()\n",
    "pixel_value = get_pixel_value(dataset, px, py)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"With GDAL, pixel value at world coordinates ({x}, {y}) is: {pixel_value}\")\n",
    "print(f\"Time taken: {elapsed_time:.6f} seconds\")\n",
    "\n",
    "# Using Rasterio\n",
    "with rasterio.open(image_file_name) as src:\n",
    "    meta = src.meta\n",
    "\n",
    "    # Use the transform in the metadata and your coordinates\n",
    "    rowcol = rasterio.transform.rowcol(meta['transform'], xs=x, ys=y)\n",
    "    start_time = time.time()\n",
    "    img = src.read(1)\n",
    "    pixel_value = img[rowcol[0], rowcol[1]]\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"With Rasterio, pixel value at world coordinates ({x}, {y}) is: {pixel_value}\")\n",
    "    print(f\"Time taken: {elapsed_time:.6f} seconds\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calculatin MORAN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pysal.lib\n",
    "from pysal.explore import esda\n",
    "from pysal.lib import weights\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Create a GeoDataFrame with some example point geometries\n",
    "geometry = [Point(xy) for xy in zip(np.random.randint(0, 100, 10), np.random.randint(0, 100, 10))]\n",
    "geo_df = gpd.GeoDataFrame(geometry=geometry)\n",
    "\n",
    "# Add an 'ID' column to the GeoDataFrame\n",
    "geo_df['ID'] = range(1, len(geo_df) + 1)\n",
    "\n",
    "# Generate some random data values for each point\n",
    "geo_df['value'] = np.random.rand(len(geo_df))\n",
    "\n",
    "# Create a spatial weights matrix (here using nearest neighbors)\n",
    "w = weights.KNN.from_dataframe(geo_df, k=3)\n",
    "\n",
    "# Compute Local Moran's I\n",
    "y = geo_df['value'].values\n",
    "local_moran = esda.Moran_Local(y, w)\n",
    "\n",
    "# Add Local Moran's I values to the GeoDataFrame\n",
    "geo_df['Local_Moran_I'] = local_moran.Is\n",
    "\n",
    "# Print coordinates, ID, and Local Moran's I values\n",
    "for idx, row in geo_df.iterrows():\n",
    "    print(f\"ID: {row['ID']}, Coordinates: {row['geometry'].coords[:][0]}, Local Moran's I: {row['Local_Moran_I']}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import libpysal as lps\n",
    "import mapclassify as mc\n",
    "import contextily as ctx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter relevant columns and unique campaigns\n",
    "subset = gdf[['Campaign','ID','geometry','precipitation_sum_31days_before']]\n",
    "campaigns = set(gdf['Campaign']) - {'Loch_Eilt_V2', 'Loch_Eilt_V3', 'Netherlands_V2'}\n",
    "\n",
    "for campaign in campaigns:\n",
    "    # Filter data for the current campaign\n",
    "    df = subset[subset[\"Campaign\"] == campaign]\n",
    "\n",
    "    # Create spatial weights matrix and transform it\n",
    "    wq = lps.weights.Queen.from_dataframe(df)\n",
    "    wq.transform = \"r\"\n",
    "\n",
    "    # Compute spatial lag and Moran's I\n",
    "    y = df['precipitation_sum_31days_before']\n",
    "    ylag = lps.weights.lag_spatial(wq, y)\n",
    "    moran = esda.Moran(y, wq)\n",
    "\n",
    "    # Update dataframe with new columns\n",
    "    df['lag_pre'] = ylag\n",
    "\n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(2.16 * 4, 4))\n",
    "    plot_args = {'edgecolor': 'k', 'scheme': 'quantiles', 'k': 3, 'cmap': 'viridis_r', 'legend': True}\n",
    "    df.plot(column='precipitation_sum_31days_before', ax=axes[0], **plot_args)\n",
    "    df.plot(column='lag_pre', ax=axes[1], **plot_args)\n",
    "\n",
    "    for ax, title in zip(axes, [f\"Precipitation in campaign {campaign}\", \"Spatial Lag Precipitation\"]):\n",
    "        ax.axis(df.total_bounds[np.asarray([0, 2, 1, 3])])\n",
    "        ax.set_title(title)\n",
    "        ctx.add_basemap(ax, crs='epsg:4326', source=ctx.providers.Stamen.TonerLite, attribution=\"\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Print Moran's I statistic\n",
    "    print(f\"Moran I: {moran.I:.2f}, p-value: {moran.p_sim:.3f}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This is a placeholder function; actual computation might be more involved\n",
    "def autocorrelation(data, dist_matrix, max_dist=10000, bin_size=100):\n",
    "    n_bins = max_dist // bin_size\n",
    "    acorr = np.zeros(n_bins)\n",
    "\n",
    "    for i in range(n_bins):\n",
    "        min_dist, max_dist = i * bin_size, (i+1) * bin_size\n",
    "        mask = (dist_matrix >= min_dist) & (dist_matrix < max_dist)\n",
    "        # Compute autocorrelation for this bin\n",
    "        acorr[i] = np.mean(data[mask])\n",
    "\n",
    "    return acorr\n",
    "\n",
    "precip_acorr = autocorrelation(precip_data['value'], dist_matrix)\n",
    "ET_acorr = autocorrelation(ET_data['value'], dist_matrix)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Closest different neighbour KDTree"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "from geopy.distance import geodesic\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_closest_different_neighbors(df, value):\n",
    "    \"\"\"\n",
    "    Find the closest pair of points in a DataFrame with different values for a given column.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame containing the points.\n",
    "        It must have columns 'lat' and 'long' for latitude and longitude, respectively.\n",
    "\n",
    "    value : str\n",
    "        The column name for which we are looking for points with different values.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        A DataFrame containing the two closest points with different values for the given column.\n",
    "        If no such pair exists, returns None.\n",
    "\n",
    "    \"\"\"\n",
    "    # Build the KD-Tree\n",
    "    tree = KDTree(df[['lat', 'long']].values)\n",
    "\n",
    "    # Initialize variables to find closest points with different values\n",
    "    min_distance = float('inf')\n",
    "    closest_points = None\n",
    "\n",
    "    # Loop through each point\n",
    "    for i, point in df.iterrows():\n",
    "        # Query the tree to find closest points\n",
    "        distances, indices = tree.query([point[['lat', 'long']]], k=len(df))\n",
    "\n",
    "        # Remove the point itself from the list of closest points\n",
    "        distances = distances[0][1:]\n",
    "        indices = indices[0][1:]\n",
    "\n",
    "        # Check values\n",
    "        for j, index in enumerate(indices):\n",
    "            if df.loc[index, value] != point[value]:\n",
    "                # Convert distances to km using Geopy's geodesic\n",
    "                point1 = (point['lat'], point['long'])\n",
    "                point2 = (df.loc[index, 'lat'], df.loc[index, 'long'])\n",
    "                distance_km = geodesic(point1, point2).km\n",
    "\n",
    "                if distance_km < min_distance:\n",
    "                    min_distance = distance_km\n",
    "                    closest_points = (i, index)\n",
    "                break\n",
    "\n",
    "    # Return results\n",
    "    if closest_points is not None:\n",
    "        closest_points_df = pd.DataFrame([df.loc[closest_points[0]], df.loc[closest_points[1]]])\n",
    "        closest_points_df['distance(km)'] = min_distance\n",
    "        return closest_points_df\n",
    "    else:\n",
    "        return None\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fully conected NN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "earlystop_patience = 150\n",
    "reducelr_patience = 100\n",
    "reducelr_factor = 0.1\n",
    "max_epochs = 1000\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=earlystop_patience, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(patience=reducelr_patience, factor=reducelr_factor)\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_met_sat.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "model.fit(X_train_met_sat, y_train_met_sat,\n",
    "          epochs=max_epochs,\n",
    "          batch_size=batch_size,\n",
    "          validation_split=0.1,\n",
    "          callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "loss, rmse = model.evaluate(X_test_met_sat, y_test_met_sat)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stacking reggressor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define base models\n",
    "base_models = [\n",
    "    ('random_forest', RandomForestRegressor(random_state=42)),\n",
    "    ('xgboost', XGBRegressor(random_state=42))\n",
    "]\n",
    "\n",
    "# Define meta-model\n",
    "meta_model = SVR(kernel='linear', C=1.0)\n",
    "\n",
    "# Create and fit the stacking regressor\n",
    "stacking_regressor = StackingRegressor(estimators=base_models, final_estimator=meta_model, cv=5)\n",
    "stacking_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = stacking_regressor.predict(X_test)\n",
    "\n",
    "# Print the R-squared score of the stacking regressor\n",
    "print(\"R-squared score of the stacking regressor: \", stacking_regressor.score(X_test, y_test))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Grid Search for XGboost regressor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_child_weight': [1, 3],\n",
    "    'subsample': [0.7, 1.0],\n",
    "    'colsample_bytree': [0.7, 1.0],\n",
    "    'n_estimators': [50, 100],\n",
    "    'reg_alpha': [0, 1.0],\n",
    "    'reg_lambda': [1, 2.0],\n",
    "    'gamma': [0, 0.5],\n",
    "}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "xg_reg = xgb.XGBRegressor()\n",
    "\n",
    "# Assuming xg_reg is your initialized XGBoost regressor\n",
    "grid_search = GridSearchCV(estimator=xg_reg, param_grid=param_grid,\n",
    "                          scoring='neg_mean_squared_error', cv=3, verbose=2,\n",
    "                          n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_search.best_score_)))\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"MSE: %f\" % (mse))\n",
    "print(\"RMSE: %f\" % (np.sqrt(mse)))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Normalize data sets for LSTM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def normalize_data(\n",
    "    train_data: pd.core.frame.DataFrame,\n",
    "    val_data: pd.core.frame.DataFrame,\n",
    "    test_data: pd.core.frame.DataFrame,\n",
    ") -> Tuple[\n",
    "    np.ndarray, np.ndarray, np.ndarray, pd.core.series.Series, pd.core.series.Series\n",
    "]:\n",
    "    \"\"\"Normalizes train, val and test splits.\n",
    "\n",
    "    Args:\n",
    "        train_data (pd.core.frame.DataFrame): Train split.\n",
    "        val_data (pd.core.frame.DataFrame): Validation split.\n",
    "        test_data (pd.core.frame.DataFrame): Test split.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Normalized splits with training mean and standard deviation.\n",
    "    \"\"\"\n",
    "    train_mean = train_data.mean()\n",
    "    train_std = train_data.std()\n",
    "\n",
    "    train_data = (train_data - train_mean) / train_std\n",
    "    val_data = (val_data - train_mean) / train_std\n",
    "    test_data = (test_data - train_mean) / train_std\n",
    "\n",
    "    return train_data, val_data, test_data, train_mean, train_std\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "plot time series"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "def plot_time_series(feature):\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 12))\n",
    "        ax1.plot(train_df[\"Patv\"], color=\"blue\", label=\"training\")\n",
    "        ax1.plot(val_df[\"Patv\"], color=\"green\", label=\"validation\")\n",
    "        ax1.plot(test_df[\"Patv\"], color=\"red\", label=\"test\")\n",
    "        ax1.set_title(\"Time series of Patv (target)\", fontsize=FONT_SIZE_TITLE)\n",
    "        ax1.set_ylabel(\"Active Power (kW)\", fontsize=FONT_SIZE_AXES)\n",
    "        ax1.set_xlabel(\"Date\", fontsize=FONT_SIZE_AXES)\n",
    "        ax1.legend(fontsize=15)\n",
    "        ax1.tick_params(axis=\"both\", labelsize=FONT_SIZE_TICKS)\n",
    "\n",
    "        ax2.plot(train_df[feature], color=\"blue\", label=\"training\")\n",
    "        ax2.plot(val_df[feature], color=\"green\", label=\"validation\")\n",
    "        ax2.plot(test_df[feature], color=\"red\", label=\"test\")\n",
    "        ax2.set_title(f\"Time series of {feature} (predictor)\", fontsize=FONT_SIZE_TITLE)\n",
    "        ax2.set_ylabel(f\"{feature}\", fontsize=FONT_SIZE_AXES)\n",
    "        ax2.set_xlabel(\"Date\", fontsize=FONT_SIZE_AXES)\n",
    "        ax2.legend(fontsize=15)\n",
    "        ax2.tick_params(axis=\"both\", labelsize=FONT_SIZE_TICKS)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    " feature_selection = widgets.Dropdown(\n",
    "        options=[f for f in list(train_df.columns) if f != \"Patv\"],\n",
    "        description=\"Feature\",\n",
    "    )\n",
    "\n",
    "interact(plot_time_series, feature=feature_selection)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "compute mmetrics between time series"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_metrics(\n",
    "    true_series: np.ndarray, forecast: np.ndarray\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Computes MSE and MAE between two time series.\n",
    "\n",
    "    Args:\n",
    "        true_series (np.ndarray): True values.\n",
    "        forecast (np.ndarray): Forecasts.\n",
    "\n",
    "    Returns:\n",
    "        tuple: MSE and MAE metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    mse = tf.keras.metrics.mean_squared_error(true_series, forecast).numpy()\n",
    "    mae = tf.keras.metrics.mean_absolute_error(true_series, forecast).numpy()\n",
    "\n",
    "    return mse, mae"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creates a Conv-LSTM model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def create_model(num_features: int, days_in_past: int) -> tf.keras.Model:\n",
    "    \"\"\"Creates a Conv-LSTM model for time series prediction.\n",
    "\n",
    "    Args:\n",
    "        num_features (int): Number of features used for prediction.\n",
    "        days_in_past (int): Number of days into the past to predict next 24 hours.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: The uncompiled model.\n",
    "    \"\"\"\n",
    "    CONV_WIDTH = 3\n",
    "    OUT_STEPS = 24\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Masking(\n",
    "                mask_value=-1.0, input_shape=(days_in_past * 24, num_features)\n",
    "            ),\n",
    "            tf.keras.layers.Lambda(lambda x: x[:, -CONV_WIDTH:, :]),\n",
    "            tf.keras.layers.Conv1D(256, activation=\"relu\", kernel_size=(CONV_WIDTH)),\n",
    "            tf.keras.layers.Bidirectional(\n",
    "                tf.keras.layers.LSTM(32, return_sequences=True)\n",
    "            ),\n",
    "            tf.keras.layers.Bidirectional(\n",
    "                tf.keras.layers.LSTM(32, return_sequences=False)\n",
    "            ),\n",
    "            tf.keras.layers.Dense(\n",
    "                OUT_STEPS * 1, kernel_initializer=tf.initializers.zeros()\n",
    "            ),\n",
    "            tf.keras.layers.Reshape([OUT_STEPS, 1]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compile and fit model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compile_and_fit(\n",
    "    model: tf.keras.Model, window: WindowGenerator, patience: int = 2\n",
    ") -> tf.keras.callbacks.History:\n",
    "    \"\"\"Compiles and trains a model given a patience threshold.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The model to train.\n",
    "        window (WindowGenerator): The windowed data.\n",
    "        patience (int, optional): Patience threshold to stop training. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.callbacks.History: The training history.\n",
    "    \"\"\"\n",
    "    EPOCHS = 20\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=patience, mode=\"min\"\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "    )\n",
    "\n",
    "    tf.random.set_seed(432)\n",
    "    np.random.seed(432)\n",
    "    random.seed(432)\n",
    "\n",
    "    history = model.fit(\n",
    "        window.train, epochs=EPOCHS, validation_data=window.val, callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    if len(history.epoch) < EPOCHS:\n",
    "        print(\"\\nTraining stopped early to prevent overfitting, as the validation loss is increasing for two consecutive steps.\")\n",
    "\n",
    "    return history\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def train_conv_lstm_model(\n",
    "    data: pd.core.frame.DataFrame, features: List[str], days_in_past: int\n",
    ") -> Tuple[WindowGenerator, tf.keras.Model, DataSplits]:\n",
    "    \"\"\"Trains the Conv-LSTM model for time series prediction.\n",
    "\n",
    "    Args:\n",
    "        data (pd.core.frame.DataFrame): The dataframe to be used.\n",
    "        data (list[str]): The features to use for forecasting.\n",
    "        days_in_past (int): How many days in the past to use to forecast the next 24 hours.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The windowed dataset, the model that handles the forecasting logic and the data used.\n",
    "    \"\"\"\n",
    "    data_splits = train_val_test_split(data[features])\n",
    "\n",
    "    train_data, val_data, test_data, train_mean, train_std = (\n",
    "        data_splits.train_data,\n",
    "        data_splits.val_data,\n",
    "        data_splits.test_data,\n",
    "        data_splits.train_mean,\n",
    "        data_splits.train_std,\n",
    "    )\n",
    "\n",
    "    window = generate_window(train_data, val_data, test_data, days_in_past)\n",
    "    num_features = window.train_df.shape[1]\n",
    "\n",
    "    model = create_model(num_features, days_in_past)\n",
    "    history = compile_and_fit(model, window)\n",
    "\n",
    "    return window, model, data_splits\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create forecast"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def add_wind_speed_forecasts(\n",
    "    df: pd.core.frame.DataFrame, add_noise=False\n",
    ") -> pd.core.frame.DataFrame:\n",
    "    \"\"\"Creates syntethic wind speed forecasts. The more into the future, the more noise these have.\n",
    "\n",
    "    Args:\n",
    "        df (pd.core.frame.DataFrame): Dataframe with data from turbine.\n",
    "        periods (list, optional): Periods for which to create the forecast. Defaults to [*range(1, 30, 1)].\n",
    "\n",
    "    Returns:\n",
    "        pd.core.frame.DataFrame: The new dataframe with the synth forecasts.\n",
    "    \"\"\"\n",
    "\n",
    "    df_2 = df.copy(deep=True)\n",
    "    # Periods for which to create the forecast.\n",
    "    periods=[*range(1, 30, 1)]\n",
    "\n",
    "    for period in periods:\n",
    "\n",
    "        if add_noise == \"linearly_increasing\":\n",
    "            np.random.seed(8752)\n",
    "            noise_level = 0.2 * period\n",
    "            noise = np.random.randn(len(df)) * noise_level\n",
    "\n",
    "        elif add_noise == \"mimic_real_forecast\":\n",
    "            np.random.seed(8752)\n",
    "            noise_level = 2 + 0.05 * period\n",
    "            noise = np.random.randn(len(df)) * noise_level\n",
    "        else:\n",
    "            noise = 0\n",
    "\n",
    "        padding_slice = df_2[\"Wspd\"][-period:].to_numpy()\n",
    "        values = np.concatenate((df_2[\"Wspd\"][period:].values, padding_slice)) + noise\n",
    "\n",
    "        df_2[f\"fc-{period}h\"] = values\n",
    "\n",
    "    return df_2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def plot_forecast_with_noise(\n",
    "    data_with_wspd_forecasts: pd.core.frame.DataFrame,\n",
    ") -> None:\n",
    "    \"\"\"Creates an interactive plot that shows how the synthetic forecasts change when the future horizon is changed.\n",
    "\n",
    "    Args:\n",
    "        data_with_wspd_forecasts (pd.core.frame.DataFrame): Dataframe that includes synth forecasts.\n",
    "    \"\"\"\n",
    "\n",
    "    def _plot(noise_level):\n",
    "        fig, ax = plt.subplots(figsize=(20, 6))\n",
    "\n",
    "        df = data_with_wspd_forecasts\n",
    "        synth_data = df[f\"fc-{noise_level}h\"][\n",
    "            5241 - noise_level : -noise_level\n",
    "        ].values\n",
    "        synth_data = tf.nn.relu(synth_data).numpy()\n",
    "        real_data = df[\"Wspd\"][5241:].values\n",
    "        real_data = tf.nn.relu(real_data).numpy()\n",
    "\n",
    "        mae = metrics.mean_absolute_error(real_data, synth_data)\n",
    "\n",
    "        print(f\"\\nMean Absolute Error (m/s): {mae:.2f} for forecast\\n\")\n",
    "        ax.plot(df.index[5241:], real_data, label=\"true values\")\n",
    "        ax.plot(\n",
    "            df.index[5241:],\n",
    "            synth_data,\n",
    "            label=\"syntethic predictions\",\n",
    "        )\n",
    "\n",
    "        ax.set_title(\"Generated wind speed forecasts\", fontsize=FONT_SIZE_TITLE)\n",
    "        ax.set_ylabel(\"Wind Speed (m/s)\", fontsize=FONT_SIZE_AXES)\n",
    "        ax.set_xlabel(\"Date\", fontsize=FONT_SIZE_AXES)\n",
    "        ax.tick_params(axis=\"both\", labelsize=FONT_SIZE_TICKS)\n",
    "        ax.legend()\n",
    "\n",
    "    noise_level_selection = widgets.IntSlider(\n",
    "        value=1,\n",
    "        min=1,\n",
    "        max=25,\n",
    "        step=1,\n",
    "        description=\"Noise level in m/s (low to high)\",\n",
    "        disabled=False,\n",
    "        continuous_update=False,\n",
    "        orientation=\"horizontal\",\n",
    "        readout=False,\n",
    "        layout={\"width\": \"500px\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "\n",
    "    interact(_plot, noise_level=noise_level_selection)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def window_plot(data_splits: DataSplits) -> None:\n",
    "    \"\"\"Creates an interactive plots to show how the data is windowed depending on the number of days into the past that are used to forecast the next 24 hours.\n",
    "\n",
    "    Args:\n",
    "        data_splits (DataSplits): Data used.\n",
    "    \"\"\"\n",
    "    train_data, val_data, test_data = (\n",
    "        data_splits.train_data,\n",
    "        data_splits.val_data,\n",
    "        data_splits.test_data,\n",
    "    )\n",
    "\n",
    "    def _plot(time_steps_past):\n",
    "        window = generate_window(train_data, val_data, test_data, time_steps_past)\n",
    "        window.plot()\n",
    "\n",
    "    time_steps_past_selection = widgets.IntSlider(\n",
    "        value=1,\n",
    "        min=1,\n",
    "        max=14,\n",
    "        step=1,\n",
    "        description=\"Days before\",\n",
    "        disabled=False,\n",
    "        continuous_update=False,\n",
    "        orientation=\"horizontal\",\n",
    "        readout=True,\n",
    "        readout_format=\"d\",\n",
    "        layout={\"width\": \"500px\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "\n",
    "    interact(_plot, time_steps_past=time_steps_past_selection)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot forecast"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_forecast(weather_forecasts: Dict[str, Dict[List[datetime], List[float]]]) -> None:\n",
    "    \"\"\"Creates an interactive plot of true values vs forecasts for the wind data.\n",
    "\n",
    "    Args:\n",
    "        weather_forecasts (dict): History of weather and weather forecasts.\n",
    "    \"\"\"\n",
    "    def _plot(city, time_steps_future):\n",
    "        format_timestamp = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "        weather_forecast = weather_forecasts[city]\n",
    "\n",
    "        dates_real, winds_real = weather_forecast[0]\n",
    "        dates_real = [datetime.strptime(i, format_timestamp) for i in dates_real]\n",
    "        dates_forecast, winds_forecast = weather_forecast[time_steps_future]\n",
    "        dates_forecast = [datetime.strptime(i, format_timestamp) for i in dates_forecast]\n",
    "\n",
    "        # Set the min and max date for plotting, so it always plots the same\n",
    "        min_date = datetime.strptime(\"2022-11-16 18:00:00\", format_timestamp)\n",
    "        max_date = datetime.strptime(\"2023-01-11 15:00:00\", format_timestamp)\n",
    "\n",
    "        # Find the overlap of the data and limit it to the plotting range\n",
    "        dates_real, dates_forecast, winds_real, winds_forecast = prepare_wind_data(\n",
    "            dates_real, dates_forecast, winds_real, winds_forecast, min_date, max_date\n",
    "        )\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(20, 6))\n",
    "        ax.plot(dates_real, winds_real, label=\"Actual windspeed\")\n",
    "        ax.plot(dates_forecast, winds_forecast, label=f\"Forecasted windspeed {time_steps_future} Hours in the Future\")\n",
    "        ax.set_title(f\"History of Actual vs Forecasted Windspeed in {city}\", fontsize=25)\n",
    "        ax.set_ylabel(\"Wind Speed (m/s)\", fontsize=20)\n",
    "        ax.set_xlabel(\"Date\", fontsize=20)\n",
    "        ax.tick_params(axis=\"both\", labelsize=15)\n",
    "        ax.legend(fontsize=15)\n",
    "\n",
    "        mae = metrics.mean_absolute_error(winds_real, winds_forecast)\n",
    "        print(f\"\\nMean Absolute Error (m/s): {mae:.2f} for forecast\\n\")\n",
    "\n",
    "    city_selection = widgets.Dropdown(\n",
    "        options=weather_forecasts.keys(),\n",
    "        description='City',\n",
    "    )\n",
    "    time_steps_future_selection = widgets.IntSlider(\n",
    "        value=1,\n",
    "        min=3,\n",
    "        max=120,\n",
    "        step=3,\n",
    "        description=\"Hours into future\",\n",
    "        disabled=False,\n",
    "        continuous_update=False,\n",
    "        orientation=\"horizontal\",\n",
    "        readout=True,\n",
    "        readout_format=\"d\",\n",
    "        layout={\"width\": \"500px\"},\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "\n",
    "    interact(_plot, city=city_selection, time_steps_future=time_steps_future_selection)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "add project to clear ml"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# in terminal first clearml-init\n",
    "from clearml import Task\n",
    "task = Task.init(project_name = \"Wind Speed Predictor\", task_name = 'tesorflow_training')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
